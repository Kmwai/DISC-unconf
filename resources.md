## References 

### Algorithms are Biased - 

* [Turns Out Algorithms Are Racist](https://newrepublic.com/article/144644/turns-algorithms-racist?utm_content=buffer7f3ea&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)

We are at a stage in technology where we need to make wide decisions of how we wish to learn from the past. Do we wish to imitate it or bring about a change. We need to challenge the inequalities that are present in the society. The first stage for this is the design process where the Data Scientist needs to understand which features are making the features bais. One should not let the black box decide and offers pronouncements and that we are encouraged to obey.

* [The Dark Secret at the Heart of AI](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)

* [Algorithms and bias: What lenders need to know](www.whitecase.com/publications/insight/algorithms-and-bias-what-lenders-need-know)

* [WHY AI IS STILL WAITING FOR ITS ETHICS TRANSPLANT](https://www.wired.com/story/why-ai-is-still-waiting-for-its-ethics-transplant/)

* [How do machines learning algorithms learn bias?](https://towardsdatascience.com/how-do-machine-learning-algorithms-learn-bias-555809a1decb)

* [Bias in software to predict future criminals - biased against blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

### Solutions - 

* [How We Examined Racial Discrimination in Auto Insurance Prices](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-methodology)

As AI becomes more and more complex, it can become difficult for even its own designers understand why it acts the way it does. A nationwide study by the Consumer Federation of America in 2015 found that predominantly African-American neighborhoods pay 70 percent more, on average, for premiums than other areas do. We analyzed aggregate risk data by zip code collected by the insurance commissioners of California, Illinois, Missouri and Texas from insurers in their states.
* They found that some insurers were charging statistically significantly higher premiums in predominantly minority zip codes, on average, than in similarly risky non-minority zip codes. 
* Studies of auto insurance rates in Texas found that “drivers in poor and minority communities were disproportionately rejected by standard insurers and forced into the higher cost non-standard” insurance plans that are designed as a last resort for people who can’t otherwise buy insurance.

Similar statistics were shown in other states as well. Insurance companies fight that it is completely based not risk and they have removed racial features while training their model but demographies are the latent features that are strongly correlated with communities. These latent features bring bais in the model.


* [How I'm Fighting Bias in Algorithms (Ted Talk)](http://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms)


 What if you go to the washroom and the tap does not respond to your hands but it does for your your friend’s. Computer Vision uses images to recognize people and objects. But what if we train it with undiversified dataset[^[1]]. It will only respond to white skin because that is only what it has seen. There have been reported indices when a webcam is not able to detect black faces.[1] Not only this image recognition softwares have been associating cleaning or the kitchen with women, for example and sports with men[^[1]].


	WHO CODES MATTERS? <sup>1</sup>
	HOW WE CODE MATTERS?[ ^ [1]]
	WHY WE CODE MATTERS?[ ^ [1]]

There needs to be a system that audits the existing software for impact that it has on minorities. There is a need to diversify our data. There is a [website](https://www.ajlunited.org/fight) where you can report discrimination that you’ve faced while using AI.

There is a website where you can report discrimination that you’ve faced while using AI
https://www.ajlunited.org/fight
 
* [Controlling machine learning algorithms and their biases](https://www.mckinsey.com/business-functions/risk/our-insights/controlling-machine-learning-algorithms-and-their-biases) 

* [Counterfactual Fairness - Causal models capture social biases and make clear the implicit trade-off between prediction
accuracy and fairness in an unfair world](https://arxiv.org/pdf/1703.06856.pdf)

* [Attacking Discrimination in ML- google research paper](https://research.google.com/bigpicture/attacking-discrimination-in-ml/)

* [Equality Opportunity](https://drive.google.com/file/d/0B-wQVEjH9yuhanpyQjUwQS1JOTQ/view)

* [Link to White Paper Containing Question Tool](https://cdt.org/issue/privacy-data/digital-decisions/)

### Involved Organizations and Communities - 

* [AI NOW](https://ainowinstitute.org/)

* [AI NOW report](https://assets.contentful.com/8wprhhvnpfc0/1A9c3ZTCZa2KEYM64Wsc2a/8636557c5fb14f2b74b2be64c3ce0c78/_AI_Now_Institute_2017_Report_.pdf)-ORG

* [DeepMind Ethics Research Group - Google](https://deepmind.com/applied/deepmind-ethics-society/research/)

  DeepMind's Ethics & Society is a research unit within DeepMind that aims to explore and better understand the real-world    
  impacts of AI. It aims to help technologists put ethics into practice and help society anticipate and control the effects 
  of AI, by enlisting some questions around:
  * privacy
  * transparency 
  * fairness 
  * governence and accountability that should be addressed throughout the life cycle of projects. 

* [Why We Launched DeepMind Ethics & Society](https://deepmind.com/blog/why-we-launched-deepmind-ethics-society/)

  Quoting DeepMind's Ethics webpage:
  
  "The development of AI creates important and complex questions. Its impact on society—and on all our lives—is not something    that should be left to chance. Beneficial outcomes and protections against harms must be actively fought for and built-in 
   from the beginning. But in a field as complex as AI, this is easier said than done"
  
  
  DeepMind being the world leader in artificial intelligence research and its application for positive impact, provides clear   motivation and need to include ethics in AI. It is imperative to have similar ethics groups working hand in hand with the     development teams in all organizations. 
   
* [Responsible Data Science](http://www.responsibledatascience.org/)

  Responsible Data Science is a novel collaboration between principal scientists from 11 universities and research institutes 
  in the Netherlands. RDS aims to provide the technology to build in fairness, accuracy, confidentiality, and transparency in 
  systems thus ensuring responsible use without inhibiting the power of data science. They conduct seminars and workshops (so 
  far within Netherlands) and the material is available on their webpage as well. 

* [principles for accountable algorithms](https://www.fatml.org/resources/principles-for-accountable-algorithms)


* [IEEE standards for AI ethics](http://standards.ieee.org/develop/indconn/ec/autonomous_systems.html)

* [Website to report bias] (https://www.ajlunited.org/fight)

* Upcoming conference-- 
https://fatconference.org/index.html
The first FAT (Fairness Accountability Transparency in socio-technical systems) conference to be held in Feb 2018 in NYC.
FAT* is an annual conference dedicating to bringing together a diverse community to investigate and tackle issues in this emerging area. Topics of interest include, but are not limited to:

The theory and practice of fair and interpretable Machine Learning, Information Retrieval, NLP, and Computer Vision
Measurement and auditing of deployed systems
Users' experience of algorithms, and design interventions to empower users
The ethical, moral, social, and policy implications of big data and ubiquitous intelligent systems

Conference on Fairness, Accountability, and Transparency - Call for Tutorials

https://fatconference.org/2018/cftutorials.html

We are soliciting calls for two types of tutorials at FAT* 2018: hands-on tutorials and translation tutorials. Both types of tutorials will give presenters 1 hour to address technical and/or policy/law FAT* issues for a broad audience.

== Important details ==

Submission deadline: December 11, 2017, 23:59 Anywhere on Earth (AoE)
Conference date: February 23-24, 2018
Conference location: New York City

== Hands-on Tutorials ==

We envision these hands-on tutorials as being a chance for a broad audience to experiment with new software packages designed to support FAT* efforts. Tutorials should introduce the motivation for the tool, explain how the underlying technology works, and walk through an example use case of the presented software. Given our emphasis on accountability and transparency, only open-source software (licensed under GPL, Apache 2.0, MIT, BSD etc.) will be expected for the tools presented in these tutorials.

Presenters may assume that participants would arrive with their own laptop to the session and would be expected to have a basic understanding of programming, though they wouldn’t necessarily be computer scientists; the tutorial should be accessible to a beginning audience.

== Translation Tutorials==

We are interested in tutorials that aim to "translate" between disciplines; by explaining computer science concepts in a way that will be practically useful for lawyers, policy makers, and other practitioners or by explaining legal, policy, or social science concepts in a way that will guide computer scientists in future technical explorations. These tutorials should be geared towards an interested, beginning audience. Tutorials should situate the topic in the related literature and do a deeper dive explaining a specific topic.

== Application Process ==

To apply to lead a tutorial, please write a 1-2 page description of the tutorial including:

        • Title - titles should be in the format, "Hands-on Tutorial: title" or "Translation Tutorial: title" depending on the type of tutorial proposed.
        • The team - we encourage tutorials to be led by one to three people. More than three people is likely to be too chaotic.
        • A description of the topic you plan to cover.
        • A short (one paragraph) timeline of how you plan to break down the material over the hour.
        • Any critical citations. Hands-on tutorials should include a link to the open source software.
        • Hands-on tutorials only: if possible, include a short code snippet and installation instructions.
        • Any specific audio/visual needs, including projectors, microphones, etc.

== Submission Instructions ==

Submissions should be emailed in PDF format to the PC Co-chairs: Sorelle Friedler (sorelle@cs.haverford.edu) and Christo Wilson (cbw@ccs.neu.edu)

Fairness, Accountability, and Transparency in Machine Learning - http://fatml.org/

* [Resources](https://fatconference.org/resources.html)

### Miscellaneous - 

* [make algorithms accountable](https://www.nytimes.com/2016/08/01/opinion/make-algorithms-accountable.html)

* [Algorithmic accountability](https://techcrunch.com/2017/04/30/algorithmic-accountability/)

* [Partnership in AI](https://www.partnershiponai.org)

* [Book: What Algorithms Want - Ed Finn](https://mitpress.mit.edu/books/what-algorithms-want)

